# ğŸ“¦ Real-Time E-Commerce Order Intelligence and Sales Monitoring (Azure-Based)

> **Live Analytics Project** using Azure Event Hub, Databricks (PySpark), Azure Blob Storage, and Power BI â€“ built entirely using U.S.-based simulated e-commerce data.

---

## ğŸš€ Project Overview

This real-time data engineering project simulates and processes live e-commerce order data to deliver insightful dashboards for U.S. retail operations. We built an end-to-end pipeline using Azure-native tools and open-source technologies, following a **Bronze â†’ Silver â†’ Gold** architecture.

ğŸ”§ **Tech Stack**:  
- **Data Ingestion**: Azure Event Hub + Python Simulator  
- **Processing**: Azure Databricks (PySpark) â€“ Bronze, Silver, Gold layers  
- **Storage**: Azure Blob Storage (Delta Format)  
- **Visualization**: Power BI Desktop (Live Connection to Gold Table)  
- **Version Control**: Git CLI & GitHub  

---

## ğŸ§ª Project Structure

```
real-time-ecommerce-insights-azure/
â”‚
â”œâ”€â”€ CICD/                    # Git version tracking setup
â”œâ”€â”€ Databrick Notebooks/    # Bronze, Silver, Gold notebooks
â”œâ”€â”€ Event Hub/              # Namespace and Hub config
â”œâ”€â”€ Power BI/               # Dashboard file (.pbix)
â”œâ”€â”€ Simulator/              # Python script to simulate U.S. orders
â”œâ”€â”€ Storage/                # Bronze, Silver, Gold Delta folders
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # ğŸ“„ This file
```

---

## ğŸ§± Step 1: Data Simulation + Event Streaming

ğŸ“¦ Created a Python simulator to continuously send fake U.S.-based e-commerce orders (TX, NY, CA...) to **Azure Event Hub**.

- Events included `order_id`, `state`, `city`, `product`, `quantity`, `price`, `timestamp`
- Used Python libraries: `uuid`, `random`, `json`, `time`, `azure-eventhub`
- Streamed events every second using a for-loop and `send_batch()` method

ğŸª› Commands:
- Used `pip install` to install Azure SDKs
- Set up Event Hub with namespace `e-commerce-namespace` and hub name `e-commerce-orders`
- Git-tracked simulator files with: `git add Simulator/`

---

## ğŸ” Step 2: Real-Time Processing in Databricks

We created a **3-layer Delta Lake architecture** using **Structured Streaming** in PySpark.

### ğŸ‚ Bronze Layer
- Read streaming data directly from Event Hub
- Parsed and flattened JSON events
- Wrote raw data to Azure Blob in Delta format

### ğŸ”§ Silver Layer
- Cleaned, filtered, and type-casted Bronze data
- Removed nulls, bad records
- Stored refined data as a Delta table

### ğŸª™ Gold Layer
- Performed aggregation with **1-minute windows** (not hourly)
- Grouped by `state`, `product`, and `timestamp`
- Stored final result table (`gold`) to Blob in Delta

ğŸ§¾ Notebooks:
- Named `1_bronze_stream.py`, `2_silver_clean.py`, `3_gold_agg.py`
- Git tracked using:  
```bash
git add Databrick\ Notebooks/
git commit -m "Add Bronze, Silver, Gold notebooks for streaming pipeline"
```

---

## ğŸ—ƒï¸ Step 3: Azure Blob Storage â€“ Delta Tables

Used Azure Blob Storage to persist all three layers in **Delta format**, organized as:

```
/mnt/data/bronze/  
/mnt/data/silver/  
/mnt/data/gold/
```

- Delta files were automatically updated via streaming write
- All Power BI visuals connected to the **Gold** layer

---

## ğŸ“Š Step 4: Power BI Dashboard (U.S. States)

Connected Power BI to **Databricks SQL Endpoint** to read the Gold Delta Table.

### ğŸ”Œ Connection Steps:
- Created SQL Warehouse in Databricks
- Connected Power BI using Azure Databricks connector (with token)
- Loaded `gold` table for visuals

### ğŸ“ˆ Visuals Created:
| Visual Type | Description |
|-------------|-------------|
| ğŸ“ Map Chart | State-wise total sales |
| ğŸ“‰ Line Chart | Sales trend over time (1-min window) |
| ğŸ© Donut Chart 1 | Total sales per state |
| ğŸ© Donut Chart 2 | Total items sold per state |
| ğŸ“„ Raw Data Table | Sorted by highest sales at top |

ğŸ§¾ Tracked Power BI with:
```bash
cd Power\ BI/
git add .
git commit -m "Add Power BI dashboard with state-wise visuals"
```

> âŒ Note: Forecasting was planned but **not implemented** in the final version.

---

## ğŸ”— Git Version Control

Used Git CLI for all version tracking:
- `git init`, `git remote add origin ...`
- `git add`, `git commit`, `git push` at every phase
- Maintained clear folder-by-folder commits

---

## âœ… Final Outcome

This project delivers a **fully functional real-time analytics pipeline** powered by Azure and Delta Lake. The dashboard provides operations teams with **minute-level insights** into U.S. e-commerce orders by product, state, and time.

---

## ğŸ“Œ Next Steps (Optional)

- Add live forecasting using Power BIâ€™s built-in analytics
- Schedule CI/CD automation for Databricks notebooks using GitHub Actions
- Extend simulator to include product categories or geolocation lat/long

---

## ğŸ“ Folder Summary

```
ğŸ“‚ Simulator         â†’ Sends real-time JSON orders to Event Hub
ğŸ“‚ Event Hub         â†’ Azure setup: namespace + hub
ğŸ“‚ Databrick Notebooks â†’ Bronze â†’ Silver â†’ Gold PySpark logic
ğŸ“‚ Storage           â†’ Delta Lake directory structure
ğŸ“‚ Power BI          â†’ Final .pbix with charts and map
ğŸ“‚ CICD              â†’ Git commit history & tracking
```

---

**Author**: *Your Name*  
**GitHub**: [yourusername](https://github.com/yourusername)

---

ğŸ¯ *Built for real-world, cloud-ready data engineering portfolios*